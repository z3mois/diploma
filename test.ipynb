{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    wn, \n",
    "    get_normal_form, \n",
    "    my_split, \n",
    "    TitleInWn,\n",
    "    is_lat\n",
    ")\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from const import DAMP_OF_WIKIDATA_PATh, GOLD_WIKIDATA_DATASET, ALL_NEED_ARTICLE\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_senses = set([' '.join([get_normal_form(w).lower() for w in s.lemma.split()]) for s in wn.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATh) if isfile(join(DAMP_OF_WIKIDATA_PATh, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATh) if isfile(join(DAMP_OF_WIKIDATA_PATh, f))]\n",
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            page = json.loads(line)\n",
    "            if (page['id'] and int(page['id'][1:]) > 10000000) or 'ru' in page['descriptions']:\n",
    "                print(page)\n",
    "                count += 1\n",
    "            if count  >= 10:\n",
    "                break           \n",
    "        if count  >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(straight_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(inverse_idx,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'rb') as f:\n",
    "#     straight_idx = pickle.load(f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'rb') as f:\n",
    "#     inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(clear_inverse_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'rb') as f:\n",
    "#     clear_inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "to_add = set()\n",
    "for file in tqdm(onlyfiles):\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if info['id'] == 'Q190':\n",
    "                    print(info)\n",
    "            if 'ru' in info['label'] and int(info['id'][1:]) <= 10000000 and (is_lat(info[\"label\"]['ru'])) and 'фильм' not in info[\"label\"]['ru']:\n",
    "                try:\n",
    "                    label = info[\"label\"]['ru']\n",
    "                    sense = wn.get_senses(label)\n",
    "                    if info['id'] == 'Q190':\n",
    "                            print(sense)        \n",
    "                    if sense:\n",
    "                        tweets.append((info, sense[0].lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            to_add.add(elem['rel_id'])\n",
    "                    else:\n",
    "                        lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                        if info['id'] == 'Q190':\n",
    "                            print(lemma, '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!') \n",
    "                        if lemma:\n",
    "                            tweets.append((info, lemma))\n",
    "                            for elem in info['rels']:\n",
    "                                to_add.add(elem['rel_id'])     \n",
    "                except:\n",
    "                    pass          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    if i != 0:\n",
    "        to_add = new_to_add\n",
    "    print(len(to_add))\n",
    "    new_to_add = set()\n",
    "    for file in onlyfiles:\n",
    "        with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                if info['id'] in to_add and int(info['id'][1:]) <= 10000000 and 'ru' in info['label']  and 'фильм' not in info[\"label\"]['ru']:\n",
    "                    try:\n",
    "                        label = info[\"label\"]['ru']\n",
    "                        sense = wn.get_senses(label)\n",
    "                        if sense:\n",
    "                            tweets.append((info, sense[0].lemma))\n",
    "                        else:\n",
    "                            lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                            if lemma is not None:\n",
    "                                tweets.append((info, lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            new_to_add.add(elem['rel_id'])\n",
    "                    except:\n",
    "                        pass\n",
    "                elif info['label'] and info['id'] in to_add:\n",
    "                    for elem in info['rels']:\n",
    "                        new_to_add.add(elem['rel_id'])\n",
    "                    tweets.append((info, info['label']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ALL_NEED_ARTICLE, 'wb') as f:\n",
    "#     pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(GOLD_WIKIDATA_DATASET)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_NEED_ARTICLE, 'rb') as f:\n",
    "    articles = pickle.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path_straight = defaultdict(set)\n",
    "graph_path_inverse = defaultdict(set)\n",
    "id_artircle2idx_article = {}\n",
    "idx_article2id_artircle = {}\n",
    "for idx, (article, _) in tqdm(enumerate(articles)):\n",
    "    id_artircle2idx_article[article['id']] = idx\n",
    "    idx_article2id_artircle[idx] = article['id']\n",
    "    for link in article['rels']:\n",
    "        graph_path_straight[link['rel_id']].add(article['id'])\n",
    "        graph_path_inverse[article['id']].add(link['rel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_vertices(graph, start_vertex):\n",
    "    visited = set()\n",
    "    queue = deque([start_vertex])\n",
    "    reachable_vertices = set()\n",
    "\n",
    "    while queue:\n",
    "        current_vertex = queue.popleft()\n",
    "        visited.add(current_vertex)\n",
    "        reachable_vertices.add(current_vertex)\n",
    "\n",
    "        for neighbor in graph[current_vertex]:\n",
    "            if neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return reachable_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_reachable_vertices(graph_path_inverse, 'Q4568'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #составляем контекст для слова из wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\asd\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.get_synsets(wn['130555-N-741490'].lemma), wn['130555-N-741490']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_lemma(all_synsets, title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_synsets:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_synsets:\n",
    "            return text[0]\n",
    "    text = title.split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word).lower()\n",
    "                for word in text])\n",
    "    if lemmatized in all_synsets:\n",
    "        return lemmatized\n",
    "    if \"ё\" in title:\n",
    "        return get_new_lemma(all_synsets, title.replace(\"ё\",\"е\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = set([lem.title.lower() for lem in wn.synsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict_candidates(articles:List[Tuple[dict, str]]) -> dict:\n",
    "    \"\"\"\n",
    "        Создание словаря из синсета в кандидаты для связывания\n",
    "    \"\"\"\n",
    "    canidates = defaultdict(list)\n",
    "    for (article, lemma) in tqdm(articles):\n",
    "        if 'ru' in article['label']:\n",
    "            if lemma:\n",
    "                synset_base_on_lemma = wn.get_synsets(lemma)\n",
    "                if synset_base_on_lemma:\n",
    "                    for synset in wn.get_synsets(lemma):\n",
    "                        canidates[synset.id].append((article, lemma))\n",
    "            else:\n",
    "                lemma_new = get_new_lemma(all_synsets, article['label']['ru'])\n",
    "                if lemma_new:\n",
    "                    for synset in wn.get_synsets(lemma_new):\n",
    "                        canidates[synset.id].append((article, lemma_new))\n",
    "    return canidates\n",
    "candidates = create_dict_candidates(articles)\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates['ЭНЦИКЛОПЕДИЧЕСКИЙ СЛОВАРЬ']\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_ctx_wikidata(article:dict) -> set:\n",
    "    '''\n",
    "        Излвечения контекста статьи из ее ооописания на русском\n",
    "    '''\n",
    "    ctx = set()\n",
    "    if 'ru' in article['descriptions']:\n",
    "        for token in clean_text(article['descriptions']['ru']).split():\n",
    "            ctx.add(get_normal_form(token).lower())\n",
    "    return ctx\n",
    "def get_score(elem1:set, elem2:set) -> float:\n",
    "    '''\n",
    "        Функция подсчета близости статьи и контекстов на основе эвристики\n",
    "    '''\n",
    "    return (len((elem1 & elem2)) + 1) / (len(elem1) + len(elem2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = defaultdict(list)\n",
    "for i, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    for candidate, lemma in candidatess:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'].lower(), lemma.lower()])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate, lemma in candidates['103410-N']:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'], lemma])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            print(candidate['id'],article_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display_synset_to_wikidata:\n",
    "    id:int\n",
    "    label:int\n",
    "    lemma:str\n",
    "    sense_id:int\n",
    "    score:float\n",
    "    synset_lemma:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_score={}\n",
    "for _, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    sorted_lst = sorted(zip(candidatess, score_dict[title_synset]), key=lambda x: x[1], reverse=True)\n",
    "    if sorted_lst and 'N' in title_synset:\n",
    "        res_score[title_synset] = display_synset_to_wikidata(sorted_lst[0][0][0]['id'], sorted_lst[0][0][0]['label'], sorted_lst[0][0][1], wn[sorted_lst[0][0][1]][0].id, sorted_lst[0][1], title_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"res.pickle\", 'wb') as f:\n",
    "#     pickle.dump(res_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res.pickle\", 'rb') as f:\n",
    "    res_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['predict_id'] = data['synset_id'].apply(lambda x: res_score[x].id if x in res_score else 'не связан')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data[data['predict_id'] !='не связан'].predict_id.astype(str), data[data['predict_id'] !='не связан'].WikiDataGoldId.astype(str)), data[data['predict_id'] !='не связан'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i in candidates else 0 for i in data['synset_id'].values]), wn['наставничество'][0].lemma, wn.get_synsets('НАСТАВНИЧЕСТВО')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_artircle2idx_article['Q7209862']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[50899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['124807-N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import read_dump, create_wikisynset,create_info_about_sense, wn, unambiguous_bindings, read_pkl\n",
    "from config.const import PATH_TO_TMP_FILE\n",
    "map = read_pkl(PATH_TO_TMP_FILE+'fst.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import read_dump, create_wikisynset,create_info_about_sense, wn, unambiguous_bindings,create_candidates_index_dict, wn, create_candidates_for_multi_stage\n",
    "from config.const import DAMP_OF_WIKIPEDIA_PATH\n",
    "# pages, dictPageRedirect, dictRedirect =read_dump(DAMP_OF_WIKIPEDIA_PATH)\n",
    "# dictWn = create_info_about_sense()\n",
    "# wiki = create_wikisynset(pages, dictPageRedirect)\n",
    "# get_sense_id_by_title('Выставка')\n",
    "# k, n = unambiguous_bindings(wn, dictWn, wiki, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading from file\n",
      "Successful reading\n",
      "read False\n",
      "Start reading from file = E:\\data_diplom\\candidates_for_multi_stage.pkl\n",
      "Successful reading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20327, 21576)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictLemmNew = create_candidates_index_dict(name='lst_candidates_after_snd_stage.pkl', mode='read')\n",
    "\n",
    "\n",
    "d = create_candidates_for_multi_stage()\n",
    "len(d), len(dictLemmNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Page(id='7', revid='636679', title='Литва', meaningPage=False, multiPage=True, categories=['Газеты Литвы', 'Архитектура Литвы', 'Всемирное наследие в Литве', 'Литва', 'Государства ', 'Государства '], links=['Германская империя', 'Литовская ССР', 'Союз Советских Социалистических Республик', 'Европа', 'Европейский союз', 'литовский язык', 'Вильнюс', 'Смешанная республика', 'Каунас', 'Клайпеда', 'Шяуляй', 'Паневежис', 'Паланга', 'Друскининкай', 'Неринга', 'Бирштонас', 'Тракай', 'Президенты Литвы', 'Гитанас Науседа', 'Ингрида Шимоните', 'Сейм Литовской Республики', 'литовцы', 'Индекс человеческого развития', 'Список стран по индексу человеческого развития', 'евро', 'Общероссийский классификатор валют', 'литовский лит', 'Восточноевропейское время', 'Летнее время', 'Восточноевропейское летнее время', 'государство', 'Северная Европа', 'Северная Европа', 'Список стран по населению', 'Список стран по территории', 'Балтийское море', 'Латвия', 'Белоруссия', 'Польша', 'Калининградская область', 'Россия', 'Прибалтика', 'Вильнюс', 'Литовский язык', 'евро', 'ООН', 'ОБСЕ', 'Совет Европы', 'Всемирная торговая организация', 'Европейский союз', 'НАТО', 'ОЭСР', 'Шенгенская зона', 'Еврозона', 'Балтийские языки', 'Славянские языки', 'Индоевропейские языки', 'топоним', 'Словакия', 'Румыния', 'Поспелов', 'Летаука', 'Повесть временных лет', 'этноним', 'Равнина', 'Ледниковый период', 'высота над уровнем моря', 'Аукштояс', 'Вильнюс', 'Неман', 'Друкшяй', 'Таурагнас', 'Дубингяй', 'Климат', 'Морской климат', 'Континентальный климат', 'Полезные ископаемые', 'торф', 'рыболовство', 'скребки', 'неолит', 'Индоевропейцы', 'земледелие', 'скотоводство', 'Индоевропейцы', 'Висла', 'Западная Двина', 'балты', 'Культура восточнолитовских курганов', 'Белоруссия', 'литовский язык', 'латышский язык', 'Кведлинбургские анналы', 'Бруно Кверфуртский', 'Русь', 'Летаука', 'Пруссия', 'Ливония', 'Миндовг', 'католицизм', 'Великое княжество Литовское', 'Западная Русь', 'православие', 'западнорусский письменный язык', 'Ягайло', 'Кревская уния', 'Королевство Польское', 'Список правителей Польши', 'Витовт', 'Гедиминовичи', 'Тевтонский орден', 'Грюнвальдская битва', 'Жемайтия', 'Казимир Ягеллон', 'Ягеллоны', 'Пруссия', 'Молдавское княжество', 'Чехия', 'Венгрия', 'Статуты Великого княжества Литовского', 'Люблин', 'Речь Посполитая', 'Люблинская уния', 'Сейм Речи Посполитой', 'шляхетская демократия', 'Шляхта', 'полонизация', 'Российская империя', 'разделы Речи Посполитой', 'Наполеон', 'запрет на литовскую латиницу', 'Книгоноши в Литве', 'Германская имперская армия', 'Первая мировая война', 'Литовская Тариба', 'Сметона', 'Акт о независимости Литвы', 'Брестский мир', 'Вильгельм фон Урах', 'Вольдемарас', 'Литовская Советская Республика', 'ССРБ', 'Литбел', 'Виленский край', 'Желиговский', 'Срединная Литва', 'Каунас', 'Сметона', 'Учредительный Сейм Литвы', 'Лига Наций', 'Университет Витовта Великого', 'Архитектурный модернизм', 'Клайпедский край', 'Прусские литовцы', 'Мемельское восстание', 'Секретный дополнительный протокол к Договору о ненападении между Германией и СССР', 'Литовская Советская Социалистическая Республика', 'ВОВ', 'Временное правительство Литвы', 'Оккупация Литвы нацистской Германией', 'рейхскомиссариат Остланд', 'генеральный округ Литва', 'Кубилюнас', 'Перестройка', 'Балтийский путь', 'Парад суверенитетов', 'Исландия', 'Государственный совет СССР', 'НАТО', 'Европейский союз', 'Еврозона', 'Староство', 'Алитусский уезд', 'Вильнюсский уезд', 'Каунасский уезд', 'Клайпедский уезд', 'Мариямпольский уезд', 'Паневежский уезд', 'Таурагский уезд', 'Тельшяйский уезд', 'Утенский уезд', 'Шяуляйский уезд', 'Местечко', 'Вильнюс', 'Каунас', 'Клайпеда', 'Шяуляй', 'депопуляция', 'эмиграция', 'естественный прирост', 'литовцы', 'поляки', 'русские', 'белорусы', 'украинцы', 'Всемирная организация здравоохранения', 'Католицизм', 'Православие', 'Государственный язык', 'литовский язык', 'Европейский союз', 'английский язык', 'Славяне', 'Поляки в Литве', 'Русские в Литве', 'Белорусы в Литве', 'Украинцы в Литве', 'билингвизм', 'польский язык', 'белорусский язык', 'диалект', 'Русский язык', 'Русский язык в Литве', 'Древнерусский язык', 'Великое княжество Литовское', 'Старообрядчество', 'Российская империя', 'парламентская республика', 'президентская республика', 'Президент Литвы', 'Президент Литовской Республики', 'Науседа', 'Парламент', 'Сейм Литовской Республики', 'Министерство юстиции Литвы', 'Конституционный суд Литвы', 'Сейм Литовской Республики', 'Порядок и справедливость', 'Союз либералов и центра', 'Движение либералов Литовской Республики', 'Избирательная акция поляков Литвы', 'парламент Литвы', 'нацистская символика', 'Символика СССР', 'флаг', 'герб', 'Третий рейх', 'Союз Советских Социалистических Республик', 'Литовская Советская Социалистическая Республика', 'Свастика', 'Серп и молот', 'Красная звезда', 'гимн', 'Иракская война', 'Литовский военный контингент в Афганистане', 'Индекс Кейтца', 'Европейский союз', 'Евростат', 'Рыночная экономика', 'инфляция', 'евро', 'ЕС', 'Еврозона', 'Европейский единый рынок', 'смешанная экономика', 'Всемирный банк', 'Страна с высоким уровнем доходов', 'Индекс глобальной конкурентоспособности', 'Всемирный экономический форум', 'НАТО', 'ЕС', 'Шенгенская зона', 'ОЭСР', 'евро', 'Балтийские тигры', 'МВФ', 'Группа Всемирного банка', 'Индекс экономической свободы', 'Прямые иностранные инвестиции', 'Ирландия', 'Сингапур', 'Пропорциональное налогообложение', 'Финтех', 'Индекс глобальной конкурентоспособности', 'Всемирный экономический форум', 'Мариямполе', 'Каунас', 'Клайпеда', 'Вильнюс', 'Паневежис', 'Мариямполе', 'Каунас', 'Железнодорожный транспорт', 'Русская колея', 'стивидор', 'Клайпеда', 'Одесса', 'Контейнерные перевозки', 'контрейлер', 'Панъевропейский транспортный коридор', 'Вильнюсский международный аэропорт', 'Шяуляйский международный аэропорт', 'Клайпедский порт', 'Литовское дворянство', 'Семенович', 'Многоступенчатая ракета', 'Вильнюсский университет', 'Форстер', 'Жилибер', 'Франк', 'Литовское дворянство', 'Великое княжество Литовское', 'Семенович', 'Ракета', 'Артиллерия', 'Многоступенчатая ракета', 'Сеземан', 'Лев Карсавин', 'Густайтис', 'Гимбутас', 'Галдикас', 'Греймас', 'Балтрушайтис', 'Кубилюс', 'Европейская комиссия', 'Лазер', 'Биотехнология', 'Астрономия', 'Каунасский технологический университет', 'Премия Кавли', 'Шикшнис', 'Европейское космическое агентство', 'Малые спутники', 'Литовский этнокосмологический музей', 'Молетская астрономическая обсерватория', 'Станкявичюс', 'космонавт', 'ЦЕРН', 'Донелайтис', 'Майронис', 'Баранаускас', 'Креве', 'Жемайте', 'Мужская сборная Литвы по баскетболу', 'Алекна', 'Баскетбол в Литве', 'Чемпионат Литвы по баскетболу', 'Мужская сборная Литвы по баскетболу', 'Чемпионат Европы по баскетболу', 'ЧМ по баскетболу', 'Вильнюсская телебашня', 'Цифровое телевидение', 'Радиовещательная станция', 'Русское радио', 'Частотная модуляция', 'Интернет', 'Департамент статистики Литвы', 'воинская обязанность', 'Поспелов', 'Одесса', 'Рига', 'Торунь'], redirect=False, first_sentence='Литва́ ( ), официальное название\\xa0— Лито́вская Респу́блика ()\\xa0— государство, расположенное в северной Европе')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['101712-N'][0].page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset(id=\"101712-N\", title=\"ЛИТВА\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.get_synset_by_id('101712-N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttt, wiki = second_stage_bindings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3043d1795ba4e0c21ad8f92575aa91bd2aeb4e429e5ce129ef86fd0e27798542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
