{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import pymorphy2\n",
    "from ruwordnet import RuWordNet\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from const import DAMP_OF_WIKIDATA_PATh, RUWORDNET_PATH\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lat(s):\n",
    "    for char in s:\n",
    "        if char.isalpha() or char.isdigit() or char == \" \" or char == \"-\" or char == ':':\n",
    "            pass\n",
    "        else:\n",
    "            return False\n",
    "    return not(s in UNICODE_EMOJI)\n",
    "@lru_cache(maxsize=200000)\n",
    "def get_normal_form(word):\n",
    "    return morph_analizer.parse(word)[0].normal_form\n",
    "morph_analizer = pymorphy2.MorphAnalyzer()\n",
    "wn = RuWordNet(filename_or_session=RUWORDNET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_senses = set([' '.join([get_normal_form(w).lower() for w in s.lemma.split()]) for s in wn.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATh) if isfile(join(DAMP_OF_WIKIDATA_PATh, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        if i!= \"(\" and i != \")\":\n",
    "            s +=i\n",
    "        elif i  == \")\":\n",
    "            s += \"\"\n",
    "        else:\n",
    "            s += \",\"\n",
    "    return s\n",
    "def TitleInWn(all_senses, title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_senses:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_senses:\n",
    "            return text[0]\n",
    "    text = my_split(title).split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word).lower()\n",
    "                for word in text[0].split()])\n",
    "    if lemmatized in all_senses:\n",
    "        return lemmatized\n",
    "    if \"ё\" in title:\n",
    "        return TitleInWn(all_senses, title.replace(\"ё\",\"е\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "to_add = set()\n",
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if 'ru' in info['label'] and (not is_lat(info[\"label\"]['ru'])):\n",
    "                label = my_split(info[\"label\"]['ru']).split(',')[0]\n",
    "                lemma = TitleInWn(set_senses, label)\n",
    "                if lemma is not None:\n",
    "                    tweets.append((info, lemma))\n",
    "                    if info['rels']:\n",
    "                        for elem in info['rels']:\n",
    "                            if elem['type'] == 'subclass_of' or elem['type'] == 'part_of':\n",
    "                                to_add.add(elem['rel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59030"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_id_to_idx = {tweet['rels'][0]['rel_id']:idx for idx, (tweet, _) in enumerate(tweets) if tweet['rels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if 'ru' in info['label'] and info['id'] in to_add:\n",
    "                label = my_split(info[\"label\"]['ru']).split(',')[0]\n",
    "                lemma = TitleInWn(set_senses, label)\n",
    "                if lemma is not None:\n",
    "                    tweets.append((info, lemma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59506"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'Q203982',\n",
       "  'pageid': 200409,\n",
       "  'label': {'en': 'Adam Małysz', 'ru': 'Малыш, Адам'},\n",
       "  'descriptions': {'en': 'Polish former ski jumper'},\n",
       "  'aliases': {'ru': ['Малыш А.', 'Адам Малыш', 'Малыш Адам', 'Adam Małysz'],\n",
       "   'en': ['Adam Malysz']},\n",
       "  'sitelinks': {'ruwiki': 'Малыш, Адам', 'enwiki': 'Adam Małysz'},\n",
       "  'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]},\n",
       " 'малыш')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3043d1795ba4e0c21ad8f92575aa91bd2aeb4e429e5ce129ef86fd0e27798542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
