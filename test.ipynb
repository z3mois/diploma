{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    wn, \n",
    "    get_normal_form, \n",
    "    my_split, \n",
    "    TitleInWn,\n",
    "    is_lat\n",
    ")\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from config.const import DAMP_OF_WIKIDATA_PATH, GOLD_WIKIDATA_DATASET, ALL_NEED_ARTICLE\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_senses = set([' '.join([get_normal_form(w).lower() for w in s.lemma.split()]) for s in wn.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATH) if isfile(join(DAMP_OF_WIKIDATA_PATH, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Q31\n",
      "{'id': 'Q31', 'pageid': 127, 'label': {'ru': '–ë–µ–ª—å–≥–∏—è', 'en': 'Belgium'}, 'descriptions': {'ru': '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ –≤ –ó–∞–ø–∞–¥–Ω–æ–π –ï–≤—Ä–æ–ø–µ', 'en': 'country in western Europe'}, 'aliases': {'en': ['Kingdom of Belgium', 'BEL', 'be', 'üáßüá™', 'BE', 'Koninkrijk Belgi√´', 'Royaume de Belgique', 'K√∂nigreich Belgien', 'Belgien'], 'ru': ['Belgique', '–ö–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–æ –ë–µ–ª—å–≥–∏—è']}, 'sitelinks': {'enwiki': 'Belgium', 'ruwiki': '–ë–µ–ª—å–≥–∏—è'}, 'rels': [{'rel_id': 'Q3624078', 'rank': 'preferred', 'type': 'instance_of'}, {'rel_id': 'Q43702', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q6256', 'rank': 'preferred', 'type': 'instance_of'}, {'rel_id': 'Q20181813', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q185441', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q1250464', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q113489728', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "8 Q8\n",
      "{'id': 'Q8', 'pageid': 134, 'label': {'en': 'happiness', 'ru': '—Å—á–∞—Å—Ç—å–µ'}, 'descriptions': {'en': 'mental or emotional state of well-being characterized by pleasant emotions', 'ru': '–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏–ª–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–µ–µ—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ —ç–º–æ—Ü–∏—è–º–∏'}, 'aliases': {'en': ['joy', 'happy', 'Happiness']}, 'sitelinks': {'enwiki': 'Happiness', 'ruwiki': '–°—á–∞—Å—Ç—å–µ'}, 'rels': [{'rel_id': 'Q331769', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q60539479', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q9415', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q16748867', 'rank': 'normal', 'type': 'subclass_of'}]}\n",
      "23 Q23\n",
      "{'id': 'Q23', 'pageid': 136, 'label': {'ru': '–î–∂–æ—Ä–¥–∂ –í–∞—à–∏–Ω–≥—Ç–æ–Ω', 'en': 'George Washington'}, 'descriptions': {'en': 'president of the United States from 1789 to 1797', 'ru': '–ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π –≤–æ–µ–Ω–Ω—ã–π –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–µ—è—Ç–µ–ª—å, –ø–µ—Ä–≤—ã–π –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê (1789‚Äì1797)'}, 'aliases': {'en': ['Father of the United States', 'The American Fabius', 'American Fabius'], 'ru': ['–í–∞—à–∏–Ω–≥—Ç–æ–Ω, –î–∂–æ—Ä–¥–∂']}, 'sitelinks': {'enwiki': 'George Washington', 'ruwiki': '–í–∞—à–∏–Ω–≥—Ç–æ–Ω, –î–∂–æ—Ä–¥–∂'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "24 Q24\n",
      "{'id': 'Q24', 'pageid': 137, 'label': {'en': 'Jack Bauer', 'ru': '–î–∂–µ–∫ –ë–∞—É—ç—Ä'}, 'descriptions': {'en': 'character from the television series 24', 'ru': '–≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–≥–æ —Ç–µ–ª–µ–≤–∏–∑–∏–æ–Ω–Ω–æ–≥–æ —à–æ—É 24'}, 'aliases': {}, 'sitelinks': {'enwiki': 'Jack Bauer'}, 'rels': [{'rel_id': 'Q15632617', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q15773317', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q20085850', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "42 Q42\n",
      "{'id': 'Q42', 'pageid': 138, 'label': {'en': 'Douglas Adams', 'ru': '–î—É–≥–ª–∞—Å –ê–¥–∞–º—Å'}, 'descriptions': {'en': 'English science fiction writer and humourist', 'ru': '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å, –¥—Ä–∞–º–∞—Ç—É—Ä–≥ –∏ —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç, –∞–≤—Ç–æ—Ä —Å–µ—Ä–∏–∏ –∫–Ω–∏–≥ ¬´–ê–≤—Ç–æ—Å—Ç–æ–ø–æ–º –ø–æ –≥–∞–ª–∞–∫—Ç–∏–∫–µ¬ª'}, 'aliases': {'en': ['Douglas No√´l Adams', 'DNA'], 'ru': ['–ê–¥–∞–º—Å, –î—É–≥–ª–∞—Å', '–î—É–≥–ª–∞—Å –ù–æ—ç–ª—å –ê–¥–∞–º—Å', '–ê–¥–∞–º—Å, –î—É–≥–ª–∞—Å –ù–æ—ç–ª—å']}, 'sitelinks': {'enwiki': 'Douglas Adams', 'ruwiki': '–ê–¥–∞–º—Å, –î—É–≥–ª–∞—Å'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "1868 Q1868\n",
      "{'id': 'Q1868', 'pageid': 142, 'label': {'en': 'Paul Otlet', 'ru': '–ü–æ–ª—å –û—Ç–ª–µ'}, 'descriptions': {'en': 'Belgian author, librarian and anti-colonial thinker', 'ru': '–±–µ–ª—å–≥–∏–π—Å–∫–∏–π –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ, –æ–¥–∏–Ω –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –£–î–ö'}, 'aliases': {'ru': ['–û—Ç–ª–µ, –ü–æ–ª—å'], 'en': ['Paul Marie Ghislain Otlet', 'Paul Marie Otlet']}, 'sitelinks': {'enwiki': 'Paul Otlet', 'ruwiki': '–û—Ç–ª–µ, –ü–æ–ª—å'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "2013 Q2013\n",
      "{'id': 'Q2013', 'pageid': 146, 'label': {'en': 'Wikidata', 'ru': '–í–∏–∫–∏–¥–∞–Ω–Ω—ã–µ'}, 'descriptions': {'en': 'free knowledge graph hosted by Wikimedia and edited by volunteers', 'ru': '–ø—Ä–æ–µ–∫—Ç –§–æ–Ω–¥–∞ –í–∏–∫–∏–º–µ–¥–∏–∞; —Å–≤–æ–±–æ–¥–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π'}, 'aliases': {'ru': ['–í–∏–∫–∏–¥–∞—Ç–∞', 'Wikidata', 'wikidata.org', 'WD', '–í–î', 'd:'], 'en': ['wikidata.org', 'm.wikidata.org', 'www.wikidata.org', 'WD', 'WKP', 'd:', 'wikidatawiki']}, 'sitelinks': {'ruwiki': '–í–∏–∫–∏–¥–∞–Ω–Ω—ã–µ', 'enwiki': 'Wikidata'}, 'rels': [{'rel_id': 'Q33120876', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q638153', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q36509592', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q15633582', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q593744', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q7094076', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q33002955', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q114955954', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q115471117', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "45 Q45\n",
      "{'id': 'Q45', 'pageid': 158, 'label': {'en': 'Portugal', 'ru': '–ü–æ—Ä—Ç—É–≥–∞–ª–∏—è'}, 'descriptions': {'en': 'country in Southwestern Europe', 'ru': '—Å—Ç—Ä–∞–Ω–∞ –≤ –ï–≤—Ä–æ–ø–µ'}, 'aliases': {'en': ['Portuguese Republic', 'PT', 'üáµüáπ', 'PRT', 'POR'], 'ru': ['Portugal', '–ü–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∞—è –†–µ—Å–ø—É–±–ª–∏–∫–∞']}, 'sitelinks': {'enwiki': 'Portugal', 'ruwiki': '–ü–æ—Ä—Ç—É–≥–∞–ª–∏—è'}, 'rels': [{'rel_id': 'Q3624078', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q6256', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q20181813', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q113489728', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "51 Q51\n",
      "{'id': 'Q51', 'pageid': 165, 'label': {'ru': '–ê–Ω—Ç–∞—Ä–∫—Ç–∏–¥–∞', 'en': 'Antarctica'}, 'descriptions': {'ru': '–∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç', 'en': 'polar continent in the Southern Hemisphere'}, 'aliases': {'en': ['Antarctic Selection']}, 'sitelinks': {'enwiki': 'Antarctica', 'ruwiki': '–ê–Ω—Ç–∞—Ä–∫—Ç–∏–¥–∞'}, 'rels': [{'rel_id': 'Q5107', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q82794', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q312461', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "58 Q58\n",
      "{'id': 'Q58', 'pageid': 178, 'label': {'en': 'penis', 'ru': '–ø–µ–Ω–∏—Å'}, 'descriptions': {'en': 'primary sexual organ of male animals', 'ru': '–æ—Ä–≥–∞–Ω —Å–æ–≤–æ–∫—É–ø–ª–µ–Ω–∏—è, —Å–ª—É–∂–∞—â–∏–π –¥–ª—è –≤–≤–µ–¥–µ–Ω–∏—è —Å–ø–µ—Ä–º—ã –≤ –ø–æ–ª–æ–≤—ã–µ –ø—É—Ç–∏ —Å–∞–º–∫–∏, —Ç–∞–∫–∂–µ –≤ —Ä—è–¥–µ —Å–ª—É—á–∞–µ–≤ —Å–ª—É–∂–∏—Ç –æ—Ä–≥–∞–Ω–æ–º –≤—ã–≤–µ–¥–µ–Ω–∏—è –º–æ—á–∏ –∏–∑ –æ—Ä–≥–∞–Ω–∏–∑–º–∞.'}, 'aliases': {'ru': ['–¥–µ—Ç–æ—Ä–æ–¥–Ω—ã–π –æ—Ä–≥–∞–Ω', '–ø–æ–ª–æ–≤–æ–π —á–ª–µ–Ω', '—á–ª–µ–Ω']}, 'sitelinks': {'enwiki': 'Penis', 'ruwiki': '–ü–æ–ª–æ–≤–æ–π —á–ª–µ–Ω'}, 'rels': [{'rel_id': 'Q712378', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q4620674', 'rank': 'normal', 'type': 'subclass_of'}, {'rel_id': 'Q168552', 'rank': 'normal', 'type': 'subclass_of'}]}\n"
     ]
    }
   ],
   "source": [
    "count  = 0\n",
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATH) if isfile(join(DAMP_OF_WIKIDATA_PATH, f))]\n",
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATH}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            page = json.loads(line)\n",
    "            if (page['id'] and int(page['id'][1:]) > 10000000) or 'ru' in page['descriptions']:\n",
    "                print(page)\n",
    "                count += 1\n",
    "            if count  >= 10:\n",
    "                break           \n",
    "        if count  >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(straight_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(inverse_idx,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'rb') as f:\n",
    "#     straight_idx = pickle.load(f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'rb') as f:\n",
    "#     inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(clear_inverse_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'rb') as f:\n",
    "#     clear_inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "to_add = set()\n",
    "for file in tqdm(onlyfiles):\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if info['id'] == 'Q190':\n",
    "                    print(info)\n",
    "            if 'ru' in info['label'] and int(info['id'][1:]) <= 10000000 and (is_lat(info[\"label\"]['ru'])) and '—Ñ–∏–ª—å–º' not in info[\"label\"]['ru']:\n",
    "                try:\n",
    "                    label = info[\"label\"]['ru']\n",
    "                    sense = wn.get_senses(label)\n",
    "                    if info['id'] == 'Q190':\n",
    "                            print(sense)        \n",
    "                    if sense:\n",
    "                        tweets.append((info, sense[0].lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            to_add.add(elem['rel_id'])\n",
    "                    else:\n",
    "                        lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                        if info['id'] == 'Q190':\n",
    "                            print(lemma, '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!') \n",
    "                        if lemma:\n",
    "                            tweets.append((info, lemma))\n",
    "                            for elem in info['rels']:\n",
    "                                to_add.add(elem['rel_id'])     \n",
    "                except:\n",
    "                    pass          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    if i != 0:\n",
    "        to_add = new_to_add\n",
    "    print(len(to_add))\n",
    "    new_to_add = set()\n",
    "    for file in onlyfiles:\n",
    "        with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                if info['id'] in to_add and int(info['id'][1:]) <= 10000000 and 'ru' in info['label']  and '—Ñ–∏–ª—å–º' not in info[\"label\"]['ru']:\n",
    "                    try:\n",
    "                        label = info[\"label\"]['ru']\n",
    "                        sense = wn.get_senses(label)\n",
    "                        if sense:\n",
    "                            tweets.append((info, sense[0].lemma))\n",
    "                        else:\n",
    "                            lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                            if lemma is not None:\n",
    "                                tweets.append((info, lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            new_to_add.add(elem['rel_id'])\n",
    "                    except:\n",
    "                        pass\n",
    "                elif info['label'] and info['id'] in to_add:\n",
    "                    for elem in info['rels']:\n",
    "                        new_to_add.add(elem['rel_id'])\n",
    "                    tweets.append((info, info['label']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ALL_NEED_ARTICLE, 'wb') as f:\n",
    "#     pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(GOLD_WIKIDATA_DATASET)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_NEED_ARTICLE, 'rb') as f:\n",
    "    articles = pickle.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path_straight = defaultdict(set)\n",
    "graph_path_inverse = defaultdict(set)\n",
    "id_artircle2idx_article = {}\n",
    "idx_article2id_artircle = {}\n",
    "for idx, (article, _) in tqdm(enumerate(articles)):\n",
    "    id_artircle2idx_article[article['id']] = idx\n",
    "    idx_article2id_artircle[idx] = article['id']\n",
    "    for link in article['rels']:\n",
    "        graph_path_straight[link['rel_id']].add(article['id'])\n",
    "        graph_path_inverse[article['id']].add(link['rel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_vertices(graph, start_vertex):\n",
    "    visited = set()\n",
    "    queue = deque([start_vertex])\n",
    "    reachable_vertices = set()\n",
    "\n",
    "    while queue:\n",
    "        current_vertex = queue.popleft()\n",
    "        visited.add(current_vertex)\n",
    "        reachable_vertices.add(current_vertex)\n",
    "\n",
    "        for neighbor in graph[current_vertex]:\n",
    "            if neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return reachable_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_reachable_vertices(graph_path_inverse, 'Q4568'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #—Å–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–≤–∞ –∏–∑ wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\asd\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.get_synsets(wn['130555-N-741490'].lemma), wn['130555-N-741490']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_lemma(all_synsets, title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(\"‚Äî\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_synsets:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_synsets:\n",
    "            return text[0]\n",
    "    text = title.split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word).lower()\n",
    "                for word in text])\n",
    "    if lemmatized in all_synsets:\n",
    "        return lemmatized\n",
    "    if \"—ë\" in title:\n",
    "        return get_new_lemma(all_synsets, title.replace(\"—ë\",\"–µ\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = set([lem.title.lower() for lem in wn.synsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict_candidates(articles:List[Tuple[dict, str]]) -> dict:\n",
    "    \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏–∑ —Å–∏–Ω—Å–µ—Ç–∞ –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è —Å–≤—è–∑—ã–≤–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    canidates = defaultdict(list)\n",
    "    for (article, lemma) in tqdm(articles):\n",
    "        if 'ru' in article['label']:\n",
    "            if lemma:\n",
    "                synset_base_on_lemma = wn.get_synsets(lemma)\n",
    "                if synset_base_on_lemma:\n",
    "                    for synset in wn.get_synsets(lemma):\n",
    "                        canidates[synset.id].append((article, lemma))\n",
    "            else:\n",
    "                lemma_new = get_new_lemma(all_synsets, article['label']['ru'])\n",
    "                if lemma_new:\n",
    "                    for synset in wn.get_synsets(lemma_new):\n",
    "                        canidates[synset.id].append((article, lemma_new))\n",
    "    return canidates\n",
    "candidates = create_dict_candidates(articles)\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates['–≠–ù–¶–ò–ö–õ–û–ü–ï–î–ò–ß–ï–°–ö–ò–ô –°–õ–û–í–ê–†–¨']\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_ctx_wikidata(article:dict) -> set:\n",
    "    '''\n",
    "        –ò–∑–ª–≤–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –∏–∑ –µ–µ –æ–æ–æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º\n",
    "    '''\n",
    "    ctx = set()\n",
    "    if 'ru' in article['descriptions']:\n",
    "        for token in clean_text(article['descriptions']['ru']).split():\n",
    "            ctx.add(get_normal_form(token).lower())\n",
    "    return ctx\n",
    "def get_score(elem1:set, elem2:set) -> float:\n",
    "    '''\n",
    "        –§—É–Ω–∫—Ü–∏—è –ø–æ–¥—Å—á–µ—Ç–∞ –±–ª–∏–∑–æ—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏\n",
    "    '''\n",
    "    return (len((elem1 & elem2)) + 1) / (len(elem1) + len(elem2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = defaultdict(list)\n",
    "for i, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    for candidate, lemma in candidatess:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'].lower(), lemma.lower()])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate, lemma in candidates['103410-N']:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'], lemma])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            print(candidate['id'],article_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display_synset_to_wikidata:\n",
    "    id:int\n",
    "    label:int\n",
    "    lemma:str\n",
    "    sense_id:int\n",
    "    score:float\n",
    "    synset_lemma:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_score={}\n",
    "for _, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    sorted_lst = sorted(zip(candidatess, score_dict[title_synset]), key=lambda x: x[1], reverse=True)\n",
    "    if sorted_lst and 'N' in title_synset:\n",
    "        res_score[title_synset] = display_synset_to_wikidata(sorted_lst[0][0][0]['id'], sorted_lst[0][0][0]['label'], sorted_lst[0][0][1], wn[sorted_lst[0][0][1]][0].id, sorted_lst[0][1], title_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"res.pickle\", 'wb') as f:\n",
    "#     pickle.dump(res_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res.pickle\", 'rb') as f:\n",
    "    res_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['predict_id'] = data['synset_id'].apply(lambda x: res_score[x].id if x in res_score else '–Ω–µ —Å–≤—è–∑–∞–Ω')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data[data['predict_id'] !='–Ω–µ —Å–≤—è–∑–∞–Ω'].predict_id.astype(str), data[data['predict_id'] !='–Ω–µ —Å–≤—è–∑–∞–Ω'].WikiDataGoldId.astype(str)), data[data['predict_id'] !='–Ω–µ —Å–≤—è–∑–∞–Ω'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i in candidates else 0 for i in data['synset_id'].values]), wn['–Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ'][0].lemma, wn.get_synsets('–ù–ê–°–¢–ê–í–ù–ò–ß–ï–°–¢–í–û')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_artircle2idx_article['Q7209862']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[50899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['124807-N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fasttext\n",
      "Version: 0.9.2\n",
      "Summary: fasttext Python bindings\n",
      "Home-page: https://github.com/facebookresearch/fastText\n",
      "Author: Onur Celebi\n",
      "Author-email: celebio@fb.com\n",
      "License: MIT\n",
      "Location: c:\\users\\professional\\anaconda3\\lib\\site-packages\n",
      "Requires: numpy, pybind11, setuptools\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('C:\\\\Users\\\\Professional\\\\Downloads\\\\cc.ru.300.bin\\\\cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import SentenceBertTransformer\n",
    "labse = SentenceBertTransformer('intfloat/multilingual-e5-large', device=\"cuda\")\n",
    "labse.load_model()\n",
    "print(labse.cosine_similarity('–ö–∞–∫ –¥–µ–ª–∞?', '–ö–∞–∫ –¥–µ–ª–∞ —É —Ç–µ–±—è'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector('—Å–ª–æ–≤–æ').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8006083369255066, '—Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ'),\n",
       " (0.7432404160499573, '—Å–ª–æ–≤–µ—á–∫–æ'),\n",
       " (0.7346797585487366, '—Å–ª–æ–≤–æ-'),\n",
       " (0.7283103466033936, '–°–ª–æ–≤–æ'),\n",
       " (0.7168722748756409, '—Å–ª–æ–≤–æ.'),\n",
       " (0.6934490203857422, '-—Å–ª–æ–≤–æ'),\n",
       " (0.6674397587776184, 'C–ª–æ–≤–æ'),\n",
       " (0.6530631184577942, '.–°–ª–æ–≤–æ'),\n",
       " (0.6497068405151367, '—Å–ª–æ–≤–æ.-'),\n",
       " (0.6411762237548828, '—Ä—É–≥–∞—Ç–µ–ª—å—Å—Ç–≤–æ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('—Å–ª–æ–≤–æ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import read_dump, create_wikisynset,create_info_about_sense, wn, unambiguous_bindings, read_pkl\n",
    "from config.const import PATH_TO_TMP_FILE\n",
    "map = read_pkl(PATH_TO_TMP_FILE+'fst.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Professional\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from src.mapWikipedia import read_dump, delete_double_in_candidates,create_info_about_sense,read_pkl, wn, second_stage_bindings,create_candidates_index_dict, wn, create_candidates_for_multi_stage,multi_bindings_stage\n",
    "from config.const import DAMP_OF_WIKIPEDIA_PATH, GOLD_DATA,PATH_TO_TMP_FILE\n",
    "# pages, dictPageRedirect, dictRedirect =read_dump(DAMP_OF_WIKIPEDIA_PATH)\n",
    "# dictWn = create_info_about_sense()\n",
    "# wiki = create_wikisynset(pages, dictPageRedirect)\n",
    "# get_sense_id_by_title('–í—ã—Å—Ç–∞–≤–∫–∞')\n",
    "# k, n = unambiguous_bindings(wn, dictWn, wiki, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file = E:\\data_diplom\\candidates_for_multi_stage.pkl\n",
      "Successful reading\n",
      "Strt deleted doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20327/20327 [00:01<00:00, 12233.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was deleted 293326 doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictWn = create_info_about_sense()\n",
    "\n",
    "dictt, new_wiki = second_stage_bindings()\n",
    "\n",
    "dictLemmNew = create_candidates_index_dict(name='lst_candidates_after_snd_stage.pkl', mode='read')\n",
    "\n",
    "\n",
    "dict_candidtes = create_candidates_for_multi_stage(new_wiki, wn, dictWn, dictLemmNew, mode='read')\n",
    "\n",
    "dict_candidtes_update = delete_double_in_candidates(dict_candidtes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6730 20327\n",
      "Start reading from file\n",
      "Successful reading\n"
     ]
    }
   ],
   "source": [
    "dicttFinal = multi_bindings_stage(dictt, dict_candidtes_update, wn, dictWn, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappp = read_pkl(path=PATH_TO_TMP_FILE+'thr_dict__labseintfloatmultilingual-e5-large.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21157"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mappp, sorted = dicttFinal[0], dicttFinal[1]\n",
    "len(mappp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"E:\\\\data_diplom\\\\snd_dict.pkl\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "single = unpickler.load()\n",
    "file.close()\n",
    "set_single = []\n",
    "for key, value in single.items():\n",
    "    set_single.append(value.wordId)\n",
    "set_single = set(set_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>annotation</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115195-N</td>\n",
       "      <td>–û–±–ª–∞–≤–∞</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–û–±–ª–∞–≤–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128158-N</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143699-N</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4061-N</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>136544-N</td>\n",
       "      <td>–®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1993</td>\n",
       "      <td>113701-N</td>\n",
       "      <td>–¶–∞—Ä—é –Ω–µ–±–µ—Å–Ω—ã–π</td>\n",
       "      <td>–ù–µ—Ç</td>\n",
       "      <td>–ë–æ–≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1994</td>\n",
       "      <td>124807-N</td>\n",
       "      <td>–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>1995</td>\n",
       "      <td>9199-N</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>1996</td>\n",
       "      <td>103410-N</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1998</td>\n",
       "      <td>141998-N</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞</td>\n",
       "      <td>–î–∞</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 synset_id              wiki_title annotation  \\\n",
       "0              0  115195-N                  –û–±–ª–∞–≤–∞         –î–∞   \n",
       "1              1  128158-N                 –®–∞–Ω—å–¥—É–Ω         –î–∞   \n",
       "2              2  143699-N               –û–∫—Å—é–º–æ—Ä–æ–Ω         –î–∞   \n",
       "3              3    4061-N            –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å         –î–∞   \n",
       "4              5  136544-N         –®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞         –î–∞   \n",
       "...          ...       ...                     ...        ...   \n",
       "1725        1993  113701-N           –¶–∞—Ä—é –Ω–µ–±–µ—Å–Ω—ã–π        –ù–µ—Ç   \n",
       "1726        1994  124807-N          –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ         –î–∞   \n",
       "1727        1995    9199-N  –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã         –î–∞   \n",
       "1728        1996  103410-N                –≠—Å–≤–∞—Ç–∏–Ω–∏         –î–∞   \n",
       "1729        1998  141998-N                –ó–µ–º–ª—è–Ω–∫–∞         –î–∞   \n",
       "\n",
       "             wiki_title_gold  \n",
       "0                     –û–±–ª–∞–≤–∞  \n",
       "1                    –®–∞–Ω—å–¥—É–Ω  \n",
       "2                  –û–∫—Å—é–º–æ—Ä–æ–Ω  \n",
       "3               –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å  \n",
       "4            –®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞  \n",
       "...                      ...  \n",
       "1725                     –ë–æ–≥  \n",
       "1726          –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ  \n",
       "1727  –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã  \n",
       "1728                –≠—Å–≤–∞—Ç–∏–Ω–∏  \n",
       "1729                –ó–µ–º–ª—è–Ω–∫–∞  \n",
       "\n",
       "[1730 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(GOLD_DATA)\n",
    "synset_id = set(df[\"synset_id\"].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_5056\\3409875238.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"–Ω–µ —Å–≤—è–∑–∞–Ω\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>–û–±–ª–∞–≤–∞</td>\n",
       "      <td>–û–±–ª–∞–≤–∞ (—Ñ–∏–ª—å–º, 2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω—Å–∫–∏–π –ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>–®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞</td>\n",
       "      <td>–Ω–µ —Å–≤—è–∑–∞–Ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>–ë–æ–≥</td>\n",
       "      <td>–î—ç–≤–∞ (–∏–Ω–¥—É–∏–∑–º)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ</td>\n",
       "      <td>–ú–µ–Ω—Ç–æ—Ä (–≥–æ—Ä–æ–¥, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞ (–ø—Ä–∏—Ç–æ–∫ –ú–∞—Ä–∞–ª–∏—Ö–∏)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                  wiki_title\n",
       "0     115195-N                  –û–±–ª–∞–≤–∞        –û–±–ª–∞–≤–∞ (—Ñ–∏–ª—å–º, 2012)\n",
       "1     128158-N                 –®–∞–Ω—å–¥—É–Ω      –®–∞–Ω—å–¥—É–Ω—Å–∫–∏–π –ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤\n",
       "2     143699-N               –û–∫—Å—é–º–æ—Ä–æ–Ω                   –û–∫—Å—é–º–æ—Ä–æ–Ω\n",
       "3       4061-N            –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å                –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\n",
       "4     136544-N         –®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞                   –Ω–µ —Å–≤—è–∑–∞–Ω\n",
       "...        ...                     ...                         ...\n",
       "1725  113701-N                     –ë–æ–≥              –î—ç–≤–∞ (–∏–Ω–¥—É–∏–∑–º)\n",
       "1726  124807-N          –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ   –ú–µ–Ω—Ç–æ—Ä (–≥–æ—Ä–æ–¥, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞)\n",
       "1727    9199-N  –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã      –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã\n",
       "1728  103410-N                –≠—Å–≤–∞—Ç–∏–Ω–∏                    –≠—Å–≤–∞—Ç–∏–Ω–∏\n",
       "1729  141998-N                –ó–µ–º–ª—è–Ω–∫–∞  –ó–µ–º–ª—è–Ω–∫–∞ (–ø—Ä–∏—Ç–æ–∫ –ú–∞—Ä–∞–ª–∏—Ö–∏)\n",
       "\n",
       "[1730 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_check = {}\n",
    "for key, value in mappp.items():\n",
    "    if value.wordId in synset_id:\n",
    "        dict_for_check[value.wordId] = value.title\n",
    "dataset = df[[\"synset_id\", \"wiki_title_gold\"]]\n",
    "for_apply = df[\"synset_id\"]\n",
    "dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"–Ω–µ —Å–≤—è–∑–∞–Ω\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5190751445086705"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = dataset[\"wiki_title\"].values\n",
    "y_true = dataset[\"wiki_title_gold\"].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"wiki_title\"]==\"–Ω–µ —Å–≤—è–∑–∞–Ω\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_5056\\183366674.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>multi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>–û–±–ª–∞–≤–∞</td>\n",
       "      <td>–û–±–ª–∞–≤–∞ (—Ñ–∏–ª—å–º, 2012)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω</td>\n",
       "      <td>–®–∞–Ω—å–¥—É–Ω—Å–∫–∏–π –ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "      <td>–û–∫—Å—é–º–æ—Ä–æ–Ω</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "      <td>–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>–®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞</td>\n",
       "      <td>–Ω–µ —Å–≤—è–∑–∞–Ω</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>–ë–æ–≥</td>\n",
       "      <td>–î—ç–≤–∞ (–∏–Ω–¥—É–∏–∑–º)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>–ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ</td>\n",
       "      <td>–ú–µ–Ω—Ç–æ—Ä (–≥–æ—Ä–æ–¥, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "      <td>–≠—Å–≤–∞—Ç–∏–Ω–∏</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞</td>\n",
       "      <td>–ó–µ–º–ª—è–Ω–∫–∞ (–ø—Ä–∏—Ç–æ–∫ –ú–∞—Ä–∞–ª–∏—Ö–∏)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                  wiki_title  multi\n",
       "0     115195-N                  –û–±–ª–∞–≤–∞        –û–±–ª–∞–≤–∞ (—Ñ–∏–ª—å–º, 2012)      1\n",
       "1     128158-N                 –®–∞–Ω—å–¥—É–Ω      –®–∞–Ω—å–¥—É–Ω—Å–∫–∏–π –ø–æ–ª—É–æ—Å—Ç—Ä–æ–≤      1\n",
       "2     143699-N               –û–∫—Å—é–º–æ—Ä–æ–Ω                   –û–∫—Å—é–º–æ—Ä–æ–Ω      0\n",
       "3       4061-N            –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å                –î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å      1\n",
       "4     136544-N         –®–∞—Ö–º–∞—Ç–Ω–∞—è –¥–æ—Å–∫–∞                   –Ω–µ —Å–≤—è–∑–∞–Ω      1\n",
       "...        ...                     ...                         ...    ...\n",
       "1725  113701-N                     –ë–æ–≥              –î—ç–≤–∞ (–∏–Ω–¥—É–∏–∑–º)      0\n",
       "1726  124807-N          –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ   –ú–µ–Ω—Ç–æ—Ä (–≥–æ—Ä–æ–¥, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞)      0\n",
       "1727    9199-N  –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã      –¢—Ä–∞–Ω—Å—É—Ä–∞–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã      1\n",
       "1728  103410-N                –≠—Å–≤–∞—Ç–∏–Ω–∏                    –≠—Å–≤–∞—Ç–∏–Ω–∏      0\n",
       "1729  141998-N                –ó–µ–º–ª—è–Ω–∫–∞  –ó–µ–º–ª—è–Ω–∫–∞ (–ø—Ä–∏—Ç–æ–∫ –ú–∞—Ä–∞–ª–∏—Ö–∏)      1\n",
       "\n",
       "[1730 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_apply = df[\"synset_id\"].copy()\n",
    "dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482035928143712"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 0, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 0, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4378531073446328"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 1, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 1, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3043d1795ba4e0c21ad8f92575aa91bd2aeb4e429e5ce129ef86fd0e27798542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
