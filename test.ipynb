{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    wn, \n",
    "    get_normal_form, \n",
    "    my_split, \n",
    "    TitleInWn,\n",
    "    is_lat\n",
    ")\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from const import DAMP_OF_WIKIDATA_PATh, GOLD_WIKIDATA_DATASET, ALL_NEED_ARTICLE\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_senses = set([' '.join([get_normal_form(w).lower() for w in s.lemma.split()]) for s in wn.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATh) if isfile(join(DAMP_OF_WIKIDATA_PATh, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATh) if isfile(join(DAMP_OF_WIKIDATA_PATh, f))]\n",
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            page = json.loads(line)\n",
    "            if (page['id'] and int(page['id'][1:]) > 10000000) or 'ru' in page['descriptions']:\n",
    "                print(page)\n",
    "                count += 1\n",
    "            if count  >= 10:\n",
    "                break           \n",
    "        if count  >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(straight_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(inverse_idx,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'rb') as f:\n",
    "#     straight_idx = pickle.load(f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'rb') as f:\n",
    "#     inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(clear_inverse_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'rb') as f:\n",
    "#     clear_inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "to_add = set()\n",
    "for file in tqdm(onlyfiles):\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if info['id'] == 'Q190':\n",
    "                    print(info)\n",
    "            if 'ru' in info['label'] and int(info['id'][1:]) <= 10000000 and (is_lat(info[\"label\"]['ru'])) and 'фильм' not in info[\"label\"]['ru']:\n",
    "                try:\n",
    "                    label = info[\"label\"]['ru']\n",
    "                    sense = wn.get_senses(label)\n",
    "                    if info['id'] == 'Q190':\n",
    "                            print(sense)        \n",
    "                    if sense:\n",
    "                        tweets.append((info, sense[0].lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            to_add.add(elem['rel_id'])\n",
    "                    else:\n",
    "                        lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                        if info['id'] == 'Q190':\n",
    "                            print(lemma, '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!') \n",
    "                        if lemma:\n",
    "                            tweets.append((info, lemma))\n",
    "                            for elem in info['rels']:\n",
    "                                to_add.add(elem['rel_id'])     \n",
    "                except:\n",
    "                    pass          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    if i != 0:\n",
    "        to_add = new_to_add\n",
    "    print(len(to_add))\n",
    "    new_to_add = set()\n",
    "    for file in onlyfiles:\n",
    "        with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                if info['id'] in to_add and int(info['id'][1:]) <= 10000000 and 'ru' in info['label']  and 'фильм' not in info[\"label\"]['ru']:\n",
    "                    try:\n",
    "                        label = info[\"label\"]['ru']\n",
    "                        sense = wn.get_senses(label)\n",
    "                        if sense:\n",
    "                            tweets.append((info, sense[0].lemma))\n",
    "                        else:\n",
    "                            lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                            if lemma is not None:\n",
    "                                tweets.append((info, lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            new_to_add.add(elem['rel_id'])\n",
    "                    except:\n",
    "                        pass\n",
    "                elif info['label'] and info['id'] in to_add:\n",
    "                    for elem in info['rels']:\n",
    "                        new_to_add.add(elem['rel_id'])\n",
    "                    tweets.append((info, info['label']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ALL_NEED_ARTICLE, 'wb') as f:\n",
    "#     pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(GOLD_WIKIDATA_DATASET)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_NEED_ARTICLE, 'rb') as f:\n",
    "    articles = pickle.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path_straight = defaultdict(set)\n",
    "graph_path_inverse = defaultdict(set)\n",
    "id_artircle2idx_article = {}\n",
    "idx_article2id_artircle = {}\n",
    "for idx, (article, _) in tqdm(enumerate(articles)):\n",
    "    id_artircle2idx_article[article['id']] = idx\n",
    "    idx_article2id_artircle[idx] = article['id']\n",
    "    for link in article['rels']:\n",
    "        graph_path_straight[link['rel_id']].add(article['id'])\n",
    "        graph_path_inverse[article['id']].add(link['rel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_vertices(graph, start_vertex):\n",
    "    visited = set()\n",
    "    queue = deque([start_vertex])\n",
    "    reachable_vertices = set()\n",
    "\n",
    "    while queue:\n",
    "        current_vertex = queue.popleft()\n",
    "        visited.add(current_vertex)\n",
    "        reachable_vertices.add(current_vertex)\n",
    "\n",
    "        for neighbor in graph[current_vertex]:\n",
    "            if neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return reachable_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_reachable_vertices(graph_path_inverse, 'Q4568'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #составляем контекст для слова из wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\asd\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.get_synsets(wn['130555-N-741490'].lemma), wn['130555-N-741490']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_lemma(all_synsets, title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_synsets:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_synsets:\n",
    "            return text[0]\n",
    "    text = title.split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word).lower()\n",
    "                for word in text])\n",
    "    if lemmatized in all_synsets:\n",
    "        return lemmatized\n",
    "    if \"ё\" in title:\n",
    "        return get_new_lemma(all_synsets, title.replace(\"ё\",\"е\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = set([lem.title.lower() for lem in wn.synsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict_candidates(articles:List[Tuple[dict, str]]) -> dict:\n",
    "    \"\"\"\n",
    "        Создание словаря из синсета в кандидаты для связывания\n",
    "    \"\"\"\n",
    "    canidates = defaultdict(list)\n",
    "    for (article, lemma) in tqdm(articles):\n",
    "        if 'ru' in article['label']:\n",
    "            if lemma:\n",
    "                synset_base_on_lemma = wn.get_synsets(lemma)\n",
    "                if synset_base_on_lemma:\n",
    "                    for synset in wn.get_synsets(lemma):\n",
    "                        canidates[synset.id].append((article, lemma))\n",
    "            else:\n",
    "                lemma_new = get_new_lemma(all_synsets, article['label']['ru'])\n",
    "                if lemma_new:\n",
    "                    for synset in wn.get_synsets(lemma_new):\n",
    "                        canidates[synset.id].append((article, lemma_new))\n",
    "    return canidates\n",
    "candidates = create_dict_candidates(articles)\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates['ЭНЦИКЛОПЕДИЧЕСКИЙ СЛОВАРЬ']\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_ctx_wikidata(article:dict) -> set:\n",
    "    '''\n",
    "        Излвечения контекста статьи из ее ооописания на русском\n",
    "    '''\n",
    "    ctx = set()\n",
    "    if 'ru' in article['descriptions']:\n",
    "        for token in clean_text(article['descriptions']['ru']).split():\n",
    "            ctx.add(get_normal_form(token).lower())\n",
    "    return ctx\n",
    "def get_score(elem1:set, elem2:set) -> float:\n",
    "    '''\n",
    "        Функция подсчета близости статьи и контекстов на основе эвристики\n",
    "    '''\n",
    "    return (len((elem1 & elem2)) + 1) / (len(elem1) + len(elem2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = defaultdict(list)\n",
    "for i, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    for candidate, lemma in candidatess:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'].lower(), lemma.lower()])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate, lemma in candidates['103410-N']:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'], lemma])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            print(candidate['id'],article_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display_synset_to_wikidata:\n",
    "    id:int\n",
    "    label:int\n",
    "    lemma:str\n",
    "    sense_id:int\n",
    "    score:float\n",
    "    synset_lemma:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_score={}\n",
    "for _, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    sorted_lst = sorted(zip(candidatess, score_dict[title_synset]), key=lambda x: x[1], reverse=True)\n",
    "    if sorted_lst and 'N' in title_synset:\n",
    "        res_score[title_synset] = display_synset_to_wikidata(sorted_lst[0][0][0]['id'], sorted_lst[0][0][0]['label'], sorted_lst[0][0][1], wn[sorted_lst[0][0][1]][0].id, sorted_lst[0][1], title_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"res.pickle\", 'wb') as f:\n",
    "#     pickle.dump(res_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res.pickle\", 'rb') as f:\n",
    "    res_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['predict_id'] = data['synset_id'].apply(lambda x: res_score[x].id if x in res_score else 'не связан')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data[data['predict_id'] !='не связан'].predict_id.astype(str), data[data['predict_id'] !='не связан'].WikiDataGoldId.astype(str)), data[data['predict_id'] !='не связан'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i in candidates else 0 for i in data['synset_id'].values]), wn['наставничество'][0].lemma, wn.get_synsets('НАСТАВНИЧЕСТВО')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_artircle2idx_article['Q7209862']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[50899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['124807-N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import read_dump, create_wikisynset,create_info_about_sense, wn, unambiguous_bindings, read_pkl\n",
    "from config.const import PATH_TO_TMP_FILE\n",
    "map = read_pkl(PATH_TO_TMP_FILE+'fst.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Professional\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from src.mapWikipedia import read_dump, delete_double_in_candidates,create_info_about_sense,read_pkl, wn, second_stage_bindings,create_candidates_index_dict, wn, create_candidates_for_multi_stage,multi_bindings_stage\n",
    "from config.const import DAMP_OF_WIKIPEDIA_PATH, GOLD_DATA,PATH_TO_TMP_FILE\n",
    "# pages, dictPageRedirect, dictRedirect =read_dump(DAMP_OF_WIKIPEDIA_PATH)\n",
    "# dictWn = create_info_about_sense()\n",
    "# wiki = create_wikisynset(pages, dictPageRedirect)\n",
    "# get_sense_id_by_title('Выставка')\n",
    "# k, n = unambiguous_bindings(wn, dictWn, wiki, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file = E:\\data_diplom\\candidates_for_multi_stage.pkl\n",
      "Successful reading\n",
      "Strt deleted doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20327/20327 [00:01<00:00, 12233.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was deleted 293326 doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictWn = create_info_about_sense()\n",
    "\n",
    "dictt, new_wiki = second_stage_bindings()\n",
    "\n",
    "dictLemmNew = create_candidates_index_dict(name='lst_candidates_after_snd_stage.pkl', mode='read')\n",
    "\n",
    "\n",
    "dict_candidtes = create_candidates_for_multi_stage(new_wiki, wn, dictWn, dictLemmNew, mode='read')\n",
    "\n",
    "dict_candidtes_update = delete_double_in_candidates(dict_candidtes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6730 20327\n",
      "Start reading from file\n",
      "Successful reading\n"
     ]
    }
   ],
   "source": [
    "dicttFinal = multi_bindings_stage(dictt, dict_candidtes_update, wn, dictWn, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappp = read_pkl(path=PATH_TO_TMP_FILE+'thr_dict_base.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20520"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mappp, sorted = dicttFinal[0], dicttFinal[1]\n",
    "len(mappp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"E:\\\\data_diplom\\\\snd_dict.pkl\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "single = unpickler.load()\n",
    "file.close()\n",
    "set_single = []\n",
    "for key, value in single.items():\n",
    "    set_single.append(value.wordId)\n",
    "set_single = set(set_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>annotation</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Да</td>\n",
       "      <td>Облава</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шаньдун</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Да</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Да</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шахматная доска</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1993</td>\n",
       "      <td>113701-N</td>\n",
       "      <td>Царю небесный</td>\n",
       "      <td>Нет</td>\n",
       "      <td>Бог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1994</td>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Да</td>\n",
       "      <td>Наставничество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>1995</td>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Да</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>1996</td>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Да</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1998</td>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Да</td>\n",
       "      <td>Землянка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 synset_id              wiki_title annotation  \\\n",
       "0              0  115195-N                  Облава         Да   \n",
       "1              1  128158-N                 Шаньдун         Да   \n",
       "2              2  143699-N               Оксюморон         Да   \n",
       "3              3    4061-N            Доверенность         Да   \n",
       "4              5  136544-N         Шахматная доска         Да   \n",
       "...          ...       ...                     ...        ...   \n",
       "1725        1993  113701-N           Царю небесный        Нет   \n",
       "1726        1994  124807-N          Наставничество         Да   \n",
       "1727        1995    9199-N  Трансурановые элементы         Да   \n",
       "1728        1996  103410-N                Эсватини         Да   \n",
       "1729        1998  141998-N                Землянка         Да   \n",
       "\n",
       "             wiki_title_gold  \n",
       "0                     Облава  \n",
       "1                    Шаньдун  \n",
       "2                  Оксюморон  \n",
       "3               Доверенность  \n",
       "4            Шахматная доска  \n",
       "...                      ...  \n",
       "1725                     Бог  \n",
       "1726          Наставничество  \n",
       "1727  Трансурановые элементы  \n",
       "1728                Эсватини  \n",
       "1729                Землянка  \n",
       "\n",
       "[1730 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(GOLD_DATA)\n",
    "synset_id = set(df[\"synset_id\"].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_7152\\3409875238.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Метод Крекера (телесериал)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Шаньдун (авианосец)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>Шахматная доска</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>Бог</td>\n",
       "      <td>Царю небесный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Ментор (город, Миннесота)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Землянка (Воронежская область)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                      wiki_title\n",
       "0     115195-N                  Облава      Метод Крекера (телесериал)\n",
       "1     128158-N                 Шаньдун             Шаньдун (авианосец)\n",
       "2     143699-N               Оксюморон                       Оксюморон\n",
       "3       4061-N            Доверенность                    Доверенность\n",
       "4     136544-N         Шахматная доска                 Шахматная доска\n",
       "...        ...                     ...                             ...\n",
       "1725  113701-N                     Бог                   Царю небесный\n",
       "1726  124807-N          Наставничество       Ментор (город, Миннесота)\n",
       "1727    9199-N  Трансурановые элементы          Трансурановые элементы\n",
       "1728  103410-N                Эсватини                        Эсватини\n",
       "1729  141998-N                Землянка  Землянка (Воронежская область)\n",
       "\n",
       "[1730 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_check = {}\n",
    "for key, value in mappp.items():\n",
    "    if value.wordId in synset_id:\n",
    "        dict_for_check[value.wordId] = value.title\n",
    "dataset = df[[\"synset_id\", \"wiki_title_gold\"]]\n",
    "for_apply = df[\"synset_id\"]\n",
    "dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49190751445086706"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = dataset[\"wiki_title\"].values\n",
    "y_true = dataset[\"wiki_title_gold\"].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"wiki_title\"]==\"не связан\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_7152\\183366674.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>multi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Метод Крекера (телесериал)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Шаньдун (авианосец)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>Бог</td>\n",
       "      <td>Царю небесный</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Ментор (город, Миннесота)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Землянка (Воронежская область)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                      wiki_title  multi\n",
       "0     115195-N                  Облава      Метод Крекера (телесериал)      1\n",
       "1     128158-N                 Шаньдун             Шаньдун (авианосец)      1\n",
       "2     143699-N               Оксюморон                       Оксюморон      0\n",
       "3       4061-N            Доверенность                    Доверенность      1\n",
       "4     136544-N         Шахматная доска                 Шахматная доска      1\n",
       "...        ...                     ...                             ...    ...\n",
       "1725  113701-N                     Бог                   Царю небесный      0\n",
       "1726  124807-N          Наставничество       Ментор (город, Миннесота)      0\n",
       "1727    9199-N  Трансурановые элементы          Трансурановые элементы      1\n",
       "1728  103410-N                Эсватини                        Эсватини      0\n",
       "1729  141998-N                Землянка  Землянка (Воронежская область)      1\n",
       "\n",
       "[1730 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_apply = df[\"synset_id\"].copy()\n",
    "dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6392215568862275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 0, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 0, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3992467043314501"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 1, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 1, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3043d1795ba4e0c21ad8f92575aa91bd2aeb4e429e5ce129ef86fd0e27798542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
