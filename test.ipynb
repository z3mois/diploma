{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    wn, \n",
    "    get_normal_form, \n",
    "    my_split, \n",
    "    TitleInWn,\n",
    "    is_lat\n",
    ")\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from config.const import DAMP_OF_WIKIDATA_PATH, GOLD_WIKIDATA_DATASET, ALL_NEED_ARTICLE\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_senses = set([' '.join([get_normal_form(w).lower() for w in s.lemma.split()]) for s in wn.senses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATH) if isfile(join(DAMP_OF_WIKIDATA_PATH, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Q31\n",
      "{'id': 'Q31', 'pageid': 127, 'label': {'ru': 'Бельгия', 'en': 'Belgium'}, 'descriptions': {'ru': 'государство в Западной Европе', 'en': 'country in western Europe'}, 'aliases': {'en': ['Kingdom of Belgium', 'BEL', 'be', '🇧🇪', 'BE', 'Koninkrijk België', 'Royaume de Belgique', 'Königreich Belgien', 'Belgien'], 'ru': ['Belgique', 'Королевство Бельгия']}, 'sitelinks': {'enwiki': 'Belgium', 'ruwiki': 'Бельгия'}, 'rels': [{'rel_id': 'Q3624078', 'rank': 'preferred', 'type': 'instance_of'}, {'rel_id': 'Q43702', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q6256', 'rank': 'preferred', 'type': 'instance_of'}, {'rel_id': 'Q20181813', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q185441', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q1250464', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q113489728', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "8 Q8\n",
      "{'id': 'Q8', 'pageid': 134, 'label': {'en': 'happiness', 'ru': 'счастье'}, 'descriptions': {'en': 'mental or emotional state of well-being characterized by pleasant emotions', 'ru': 'ментальное или эмоциональное состояние благополучия, характеризующееся положительными эмоциями'}, 'aliases': {'en': ['joy', 'happy', 'Happiness']}, 'sitelinks': {'enwiki': 'Happiness', 'ruwiki': 'Счастье'}, 'rels': [{'rel_id': 'Q331769', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q60539479', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q9415', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q16748867', 'rank': 'normal', 'type': 'subclass_of'}]}\n",
      "23 Q23\n",
      "{'id': 'Q23', 'pageid': 136, 'label': {'ru': 'Джордж Вашингтон', 'en': 'George Washington'}, 'descriptions': {'en': 'president of the United States from 1789 to 1797', 'ru': 'Американский военный и политический деятель, первый президент США (1789–1797)'}, 'aliases': {'en': ['Father of the United States', 'The American Fabius', 'American Fabius'], 'ru': ['Вашингтон, Джордж']}, 'sitelinks': {'enwiki': 'George Washington', 'ruwiki': 'Вашингтон, Джордж'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "24 Q24\n",
      "{'id': 'Q24', 'pageid': 137, 'label': {'en': 'Jack Bauer', 'ru': 'Джек Бауэр'}, 'descriptions': {'en': 'character from the television series 24', 'ru': 'главный герой американского телевизионного шоу 24'}, 'aliases': {}, 'sitelinks': {'enwiki': 'Jack Bauer'}, 'rels': [{'rel_id': 'Q15632617', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q15773317', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q20085850', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "42 Q42\n",
      "{'id': 'Q42', 'pageid': 138, 'label': {'en': 'Douglas Adams', 'ru': 'Дуглас Адамс'}, 'descriptions': {'en': 'English science fiction writer and humourist', 'ru': 'английский писатель, драматург и сценарист, автор серии книг «Автостопом по галактике»'}, 'aliases': {'en': ['Douglas Noël Adams', 'DNA'], 'ru': ['Адамс, Дуглас', 'Дуглас Ноэль Адамс', 'Адамс, Дуглас Ноэль']}, 'sitelinks': {'enwiki': 'Douglas Adams', 'ruwiki': 'Адамс, Дуглас'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "1868 Q1868\n",
      "{'id': 'Q1868', 'pageid': 142, 'label': {'en': 'Paul Otlet', 'ru': 'Поль Отле'}, 'descriptions': {'en': 'Belgian author, librarian and anti-colonial thinker', 'ru': 'бельгийский библиограф, один из создателей УДК'}, 'aliases': {'ru': ['Отле, Поль'], 'en': ['Paul Marie Ghislain Otlet', 'Paul Marie Otlet']}, 'sitelinks': {'enwiki': 'Paul Otlet', 'ruwiki': 'Отле, Поль'}, 'rels': [{'rel_id': 'Q5', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "2013 Q2013\n",
      "{'id': 'Q2013', 'pageid': 146, 'label': {'en': 'Wikidata', 'ru': 'Викиданные'}, 'descriptions': {'en': 'free knowledge graph hosted by Wikimedia and edited by volunteers', 'ru': 'проект Фонда Викимедиа; свободная база знаний, которую может редактировать каждый'}, 'aliases': {'ru': ['Викидата', 'Wikidata', 'wikidata.org', 'WD', 'ВД', 'd:'], 'en': ['wikidata.org', 'm.wikidata.org', 'www.wikidata.org', 'WD', 'WKP', 'd:', 'wikidatawiki']}, 'sitelinks': {'ruwiki': 'Викиданные', 'enwiki': 'Wikidata'}, 'rels': [{'rel_id': 'Q33120876', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q638153', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q36509592', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q15633582', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q593744', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q7094076', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q33002955', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q114955954', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q115471117', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "45 Q45\n",
      "{'id': 'Q45', 'pageid': 158, 'label': {'en': 'Portugal', 'ru': 'Португалия'}, 'descriptions': {'en': 'country in Southwestern Europe', 'ru': 'страна в Европе'}, 'aliases': {'en': ['Portuguese Republic', 'PT', '🇵🇹', 'PRT', 'POR'], 'ru': ['Portugal', 'Португальская Республика']}, 'sitelinks': {'enwiki': 'Portugal', 'ruwiki': 'Португалия'}, 'rels': [{'rel_id': 'Q3624078', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q6256', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q20181813', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q113489728', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "51 Q51\n",
      "{'id': 'Q51', 'pageid': 165, 'label': {'ru': 'Антарктида', 'en': 'Antarctica'}, 'descriptions': {'ru': 'континент', 'en': 'polar continent in the Southern Hemisphere'}, 'aliases': {'en': ['Antarctic Selection']}, 'sitelinks': {'enwiki': 'Antarctica', 'ruwiki': 'Антарктида'}, 'rels': [{'rel_id': 'Q5107', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q82794', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q312461', 'rank': 'normal', 'type': 'instance_of'}]}\n",
      "58 Q58\n",
      "{'id': 'Q58', 'pageid': 178, 'label': {'en': 'penis', 'ru': 'пенис'}, 'descriptions': {'en': 'primary sexual organ of male animals', 'ru': 'орган совокупления, служащий для введения спермы в половые пути самки, также в ряде случаев служит органом выведения мочи из организма.'}, 'aliases': {'ru': ['детородный орган', 'половой член', 'член']}, 'sitelinks': {'enwiki': 'Penis', 'ruwiki': 'Половой член'}, 'rels': [{'rel_id': 'Q712378', 'rank': 'normal', 'type': 'instance_of'}, {'rel_id': 'Q4620674', 'rank': 'normal', 'type': 'subclass_of'}, {'rel_id': 'Q168552', 'rank': 'normal', 'type': 'subclass_of'}]}\n"
     ]
    }
   ],
   "source": [
    "count  = 0\n",
    "onlyfiles = [f for f in listdir(DAMP_OF_WIKIDATA_PATH) if isfile(join(DAMP_OF_WIKIDATA_PATH, f))]\n",
    "for file in onlyfiles:\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATH}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            page = json.loads(line)\n",
    "            if (page['id'] and int(page['id'][1:]) > 10000000) or 'ru' in page['descriptions']:\n",
    "                print(page)\n",
    "                count += 1\n",
    "            if count  >= 10:\n",
    "                break           \n",
    "        if count  >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(straight_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(inverse_idx,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\all_straight_graph.pickle\", 'rb') as f:\n",
    "#     straight_idx = pickle.load(f)\n",
    "# with open(\"D:\\\\lbase_data\\\\all_inverse_graph.pickle\", 'rb') as f:\n",
    "#     inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'wb') as f:\n",
    "#     pickle.dump(clear_inverse_idx,f)\n",
    "# with open(\"D:\\\\lbase_data\\\\clear_inverse_graph.pickle\", 'rb') as f:\n",
    "#     clear_inverse_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "to_add = set()\n",
    "for file in tqdm(onlyfiles):\n",
    "    with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            info = json.loads(line)\n",
    "            if info['id'] == 'Q190':\n",
    "                    print(info)\n",
    "            if 'ru' in info['label'] and int(info['id'][1:]) <= 10000000 and (is_lat(info[\"label\"]['ru'])) and 'фильм' not in info[\"label\"]['ru']:\n",
    "                try:\n",
    "                    label = info[\"label\"]['ru']\n",
    "                    sense = wn.get_senses(label)\n",
    "                    if info['id'] == 'Q190':\n",
    "                            print(sense)        \n",
    "                    if sense:\n",
    "                        tweets.append((info, sense[0].lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            to_add.add(elem['rel_id'])\n",
    "                    else:\n",
    "                        lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                        if info['id'] == 'Q190':\n",
    "                            print(lemma, '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!') \n",
    "                        if lemma:\n",
    "                            tweets.append((info, lemma))\n",
    "                            for elem in info['rels']:\n",
    "                                to_add.add(elem['rel_id'])     \n",
    "                except:\n",
    "                    pass          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    if i != 0:\n",
    "        to_add = new_to_add\n",
    "    print(len(to_add))\n",
    "    new_to_add = set()\n",
    "    for file in onlyfiles:\n",
    "        with open(f'{DAMP_OF_WIKIDATA_PATh}\\\\{file}', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                if info['id'] in to_add and int(info['id'][1:]) <= 10000000 and 'ru' in info['label']  and 'фильм' not in info[\"label\"]['ru']:\n",
    "                    try:\n",
    "                        label = info[\"label\"]['ru']\n",
    "                        sense = wn.get_senses(label)\n",
    "                        if sense:\n",
    "                            tweets.append((info, sense[0].lemma))\n",
    "                        else:\n",
    "                            lemma = TitleInWn(set_senses, my_split(label).split(',')[0])\n",
    "                            if lemma is not None:\n",
    "                                tweets.append((info, lemma))\n",
    "                        for elem in info['rels']:\n",
    "                            new_to_add.add(elem['rel_id'])\n",
    "                    except:\n",
    "                        pass\n",
    "                elif info['label'] and info['id'] in to_add:\n",
    "                    for elem in info['rels']:\n",
    "                        new_to_add.add(elem['rel_id'])\n",
    "                    tweets.append((info, info['label']['en']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ALL_NEED_ARTICLE, 'wb') as f:\n",
    "#     pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(GOLD_WIKIDATA_DATASET)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ALL_NEED_ARTICLE, 'rb') as f:\n",
    "    articles = pickle.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path_straight = defaultdict(set)\n",
    "graph_path_inverse = defaultdict(set)\n",
    "id_artircle2idx_article = {}\n",
    "idx_article2id_artircle = {}\n",
    "for idx, (article, _) in tqdm(enumerate(articles)):\n",
    "    id_artircle2idx_article[article['id']] = idx\n",
    "    idx_article2id_artircle[idx] = article['id']\n",
    "    for link in article['rels']:\n",
    "        graph_path_straight[link['rel_id']].add(article['id'])\n",
    "        graph_path_inverse[article['id']].add(link['rel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_vertices(graph, start_vertex):\n",
    "    visited = set()\n",
    "    queue = deque([start_vertex])\n",
    "    reachable_vertices = set()\n",
    "\n",
    "    while queue:\n",
    "        current_vertex = queue.popleft()\n",
    "        visited.add(current_vertex)\n",
    "        reachable_vertices.add(current_vertex)\n",
    "\n",
    "        for neighbor in graph[current_vertex]:\n",
    "            if neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return reachable_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_reachable_vertices(graph_path_inverse, 'Q4568'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCtxS(wn, lemma:str):\n",
    "    #составляем контекст для слова из wordnet\n",
    "    ctx_s = set()\n",
    "    #synonymy\n",
    "    for sense in wn.get_synsets(lemma):\n",
    "        for synonymy in sense.senses:\n",
    "            ctx_s.update(my_split(synonymy.lemma).split(\",\"))\n",
    "    #Hypernymy/Hyponymy\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            ctx_s.update(my_split(hypernyms.title).split(\",\"))\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hyponyms in sense.synset.hyponyms:\n",
    "            ctx_s.update(my_split(hyponyms.title).split(\",\"))\n",
    "    #Sisterhood:\n",
    "    for sense in wn.get_senses(lemma):\n",
    "        for hypernyms in sense.synset.hypernyms:\n",
    "            for sister in hypernyms.hyponyms:\n",
    "                ctx_s.update(my_split(sister.title).split(\",\"))\n",
    "    return ctx_s\n",
    "@dataclass\n",
    "class WnCtx:\n",
    "    id: int\n",
    "    ctx: set\n",
    "    lemmaInWn: str\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\\\asd\\\\ctxS.txt\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "dictWn = unpickler.load()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.get_synsets(wn['130555-N-741490'].lemma), wn['130555-N-741490']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_lemma(all_synsets, title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(\"—\", \"-\")\n",
    "    title = title.replace(\",\", \"\")\n",
    "    if title in all_synsets:\n",
    "        return title\n",
    "    if \"(\" in title:\n",
    "        text = my_split(title).split(\",\")\n",
    "        if text[0] in all_synsets:\n",
    "            return text[0]\n",
    "    text = title.split(\",\")\n",
    "    lemmatized = \" \".join([get_normal_form(word).lower()\n",
    "                for word in text])\n",
    "    if lemmatized in all_synsets:\n",
    "        return lemmatized\n",
    "    if \"ё\" in title:\n",
    "        return get_new_lemma(all_synsets, title.replace(\"ё\",\"е\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synsets = set([lem.title.lower() for lem in wn.synsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict_candidates(articles:List[Tuple[dict, str]]) -> dict:\n",
    "    \"\"\"\n",
    "        Создание словаря из синсета в кандидаты для связывания\n",
    "    \"\"\"\n",
    "    canidates = defaultdict(list)\n",
    "    for (article, lemma) in tqdm(articles):\n",
    "        if 'ru' in article['label']:\n",
    "            if lemma:\n",
    "                synset_base_on_lemma = wn.get_synsets(lemma)\n",
    "                if synset_base_on_lemma:\n",
    "                    for synset in wn.get_synsets(lemma):\n",
    "                        canidates[synset.id].append((article, lemma))\n",
    "            else:\n",
    "                lemma_new = get_new_lemma(all_synsets, article['label']['ru'])\n",
    "                if lemma_new:\n",
    "                    for synset in wn.get_synsets(lemma_new):\n",
    "                        canidates[synset.id].append((article, lemma_new))\n",
    "    return canidates\n",
    "candidates = create_dict_candidates(articles)\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates['ЭНЦИКЛОПЕДИЧЕСКИЙ СЛОВАРЬ']\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_ctx_wikidata(article:dict) -> set:\n",
    "    '''\n",
    "        Излвечения контекста статьи из ее ооописания на русском\n",
    "    '''\n",
    "    ctx = set()\n",
    "    if 'ru' in article['descriptions']:\n",
    "        for token in clean_text(article['descriptions']['ru']).split():\n",
    "            ctx.add(get_normal_form(token).lower())\n",
    "    return ctx\n",
    "def get_score(elem1:set, elem2:set) -> float:\n",
    "    '''\n",
    "        Функция подсчета близости статьи и контекстов на основе эвристики\n",
    "    '''\n",
    "    return (len((elem1 & elem2)) + 1) / (len(elem1) + len(elem2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = defaultdict(list)\n",
    "for i, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    for candidate, lemma in candidatess:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'].lower(), lemma.lower()])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate, lemma in candidates['103410-N']:\n",
    "        if 'N' in wn[lemma][0].id:\n",
    "            synset_ctx = dictWn[wn[lemma][0].id].ctx\n",
    "            article_ctx = extract_ctx_wikidata(candidate)\n",
    "            article_ctx.update([candidate['label']['ru'], lemma])\n",
    "            score = get_score(article_ctx, synset_ctx)\n",
    "            print(candidate['id'],article_ctx)\n",
    "            score_dict[title_synset].append(score)\n",
    "        else:\n",
    "            score_dict[title_synset].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class display_synset_to_wikidata:\n",
    "    id:int\n",
    "    label:int\n",
    "    lemma:str\n",
    "    sense_id:int\n",
    "    score:float\n",
    "    synset_lemma:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_score={}\n",
    "for _, (title_synset, candidatess) in tqdm(enumerate(candidates.items())):\n",
    "    sorted_lst = sorted(zip(candidatess, score_dict[title_synset]), key=lambda x: x[1], reverse=True)\n",
    "    if sorted_lst and 'N' in title_synset:\n",
    "        res_score[title_synset] = display_synset_to_wikidata(sorted_lst[0][0][0]['id'], sorted_lst[0][0][0]['label'], sorted_lst[0][0][1], wn[sorted_lst[0][0][1]][0].id, sorted_lst[0][1], title_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"res.pickle\", 'wb') as f:\n",
    "#     pickle.dump(res_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res.pickle\", 'rb') as f:\n",
    "    res_score = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['predict_id'] = data['synset_id'].apply(lambda x: res_score[x].id if x in res_score else 'не связан')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data[data['predict_id'] !='не связан'].predict_id.astype(str), data[data['predict_id'] !='не связан'].WikiDataGoldId.astype(str)), data[data['predict_id'] !='не связан'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i in candidates else 0 for i in data['synset_id'].values]), wn['наставничество'][0].lemma, wn.get_synsets('НАСТАВНИЧЕСТВО')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_artircle2idx_article['Q7209862']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[50899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['124807-N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fasttext\n",
      "Version: 0.9.2\n",
      "Summary: fasttext Python bindings\n",
      "Home-page: https://github.com/facebookresearch/fastText\n",
      "Author: Onur Celebi\n",
      "Author-email: celebio@fb.com\n",
      "License: MIT\n",
      "Location: c:\\users\\professional\\anaconda3\\lib\\site-packages\n",
      "Requires: numpy, pybind11, setuptools\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('C:\\\\Users\\\\Professional\\\\Downloads\\\\cc.ru.300.bin\\\\cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import SentenceBertTransformer\n",
    "labse = SentenceBertTransformer('intfloat/multilingual-e5-large', device=\"cuda\")\n",
    "labse.load_model()\n",
    "print(labse.cosine_similarity('Как дела?', 'Как дела у тебя'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector('слово').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8006083369255066, 'словосочетание'),\n",
       " (0.7432404160499573, 'словечко'),\n",
       " (0.7346797585487366, 'слово-'),\n",
       " (0.7283103466033936, 'Слово'),\n",
       " (0.7168722748756409, 'слово.'),\n",
       " (0.6934490203857422, '-слово'),\n",
       " (0.6674397587776184, 'Cлово'),\n",
       " (0.6530631184577942, '.Слово'),\n",
       " (0.6497068405151367, 'слово.-'),\n",
       " (0.6411762237548828, 'ругательство')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('слово')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mapWikipedia import read_dump, create_wikisynset,create_info_about_sense, wn, unambiguous_bindings, read_pkl\n",
    "from config.const import PATH_TO_TMP_FILE\n",
    "map = read_pkl(PATH_TO_TMP_FILE+'fst.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Professional\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from src.mapWikipedia import read_dump, delete_double_in_candidates,create_info_about_sense,read_pkl, wn, second_stage_bindings,create_candidates_index_dict, wn, create_candidates_for_multi_stage,multi_bindings_stage\n",
    "from config.const import DAMP_OF_WIKIPEDIA_PATH, GOLD_DATA,PATH_TO_TMP_FILE\n",
    "# pages, dictPageRedirect, dictRedirect =read_dump(DAMP_OF_WIKIPEDIA_PATH)\n",
    "# dictWn = create_info_about_sense()\n",
    "# wiki = create_wikisynset(pages, dictPageRedirect)\n",
    "# get_sense_id_by_title('Выставка')\n",
    "# k, n = unambiguous_bindings(wn, dictWn, wiki, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file\n",
      "Successful reading\n",
      "Start reading from file = E:\\data_diplom\\candidates_for_multi_stage.pkl\n",
      "Successful reading\n",
      "Strt deleted doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20327/20327 [00:01<00:00, 12233.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was deleted 293326 doubles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictWn = create_info_about_sense()\n",
    "\n",
    "dictt, new_wiki = second_stage_bindings()\n",
    "\n",
    "dictLemmNew = create_candidates_index_dict(name='lst_candidates_after_snd_stage.pkl', mode='read')\n",
    "\n",
    "\n",
    "dict_candidtes = create_candidates_for_multi_stage(new_wiki, wn, dictWn, dictLemmNew, mode='read')\n",
    "\n",
    "dict_candidtes_update = delete_double_in_candidates(dict_candidtes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6730 20327\n",
      "Start reading from file\n",
      "Successful reading\n"
     ]
    }
   ],
   "source": [
    "dicttFinal = multi_bindings_stage(dictt, dict_candidtes_update, wn, dictWn, mode='read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappp = read_pkl(path=PATH_TO_TMP_FILE+'thr_dict__labseintfloatmultilingual-e5-large.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21157"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mappp, sorted = dicttFinal[0], dicttFinal[1]\n",
    "len(mappp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"E:\\\\data_diplom\\\\snd_dict.pkl\", \"rb\")\n",
    "unpickler = pickle.Unpickler(file)\n",
    "single = unpickler.load()\n",
    "file.close()\n",
    "set_single = []\n",
    "for key, value in single.items():\n",
    "    set_single.append(value.wordId)\n",
    "set_single = set(set_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>annotation</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Да</td>\n",
       "      <td>Облава</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шаньдун</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Да</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Да</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>Да</td>\n",
       "      <td>Шахматная доска</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1993</td>\n",
       "      <td>113701-N</td>\n",
       "      <td>Царю небесный</td>\n",
       "      <td>Нет</td>\n",
       "      <td>Бог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1994</td>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Да</td>\n",
       "      <td>Наставничество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>1995</td>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Да</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>1996</td>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Да</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1998</td>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Да</td>\n",
       "      <td>Землянка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 synset_id              wiki_title annotation  \\\n",
       "0              0  115195-N                  Облава         Да   \n",
       "1              1  128158-N                 Шаньдун         Да   \n",
       "2              2  143699-N               Оксюморон         Да   \n",
       "3              3    4061-N            Доверенность         Да   \n",
       "4              5  136544-N         Шахматная доска         Да   \n",
       "...          ...       ...                     ...        ...   \n",
       "1725        1993  113701-N           Царю небесный        Нет   \n",
       "1726        1994  124807-N          Наставничество         Да   \n",
       "1727        1995    9199-N  Трансурановые элементы         Да   \n",
       "1728        1996  103410-N                Эсватини         Да   \n",
       "1729        1998  141998-N                Землянка         Да   \n",
       "\n",
       "             wiki_title_gold  \n",
       "0                     Облава  \n",
       "1                    Шаньдун  \n",
       "2                  Оксюморон  \n",
       "3               Доверенность  \n",
       "4            Шахматная доска  \n",
       "...                      ...  \n",
       "1725                     Бог  \n",
       "1726          Наставничество  \n",
       "1727  Трансурановые элементы  \n",
       "1728                Эсватини  \n",
       "1729                Землянка  \n",
       "\n",
       "[1730 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(GOLD_DATA)\n",
    "synset_id = set(df[\"synset_id\"].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_5056\\3409875238.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Облава (фильм, 2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Шаньдунский полуостров</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Оксюморон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Доверенность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>не связан</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>Бог</td>\n",
       "      <td>Дэва (индуизм)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Ментор (город, Миннесота)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Эсватини</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Землянка (приток Маралихи)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                  wiki_title\n",
       "0     115195-N                  Облава        Облава (фильм, 2012)\n",
       "1     128158-N                 Шаньдун      Шаньдунский полуостров\n",
       "2     143699-N               Оксюморон                   Оксюморон\n",
       "3       4061-N            Доверенность                Доверенность\n",
       "4     136544-N         Шахматная доска                   не связан\n",
       "...        ...                     ...                         ...\n",
       "1725  113701-N                     Бог              Дэва (индуизм)\n",
       "1726  124807-N          Наставничество   Ментор (город, Миннесота)\n",
       "1727    9199-N  Трансурановые элементы      Трансурановые элементы\n",
       "1728  103410-N                Эсватини                    Эсватини\n",
       "1729  141998-N                Землянка  Землянка (приток Маралихи)\n",
       "\n",
       "[1730 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_check = {}\n",
    "for key, value in mappp.items():\n",
    "    if value.wordId in synset_id:\n",
    "        dict_for_check[value.wordId] = value.title\n",
    "dataset = df[[\"synset_id\", \"wiki_title_gold\"]]\n",
    "for_apply = df[\"synset_id\"]\n",
    "dataset[\"wiki_title\"] = for_apply.apply(lambda x: dict_for_check[x] if x in dict_for_check else \"не связан\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5190751445086705"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = dataset[\"wiki_title\"].values\n",
    "y_true = dataset[\"wiki_title_gold\"].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"wiki_title\"]==\"не связан\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Professional\\AppData\\Local\\Temp\\ipykernel_5056\\183366674.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset_id</th>\n",
       "      <th>wiki_title_gold</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>multi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115195-N</td>\n",
       "      <td>Облава</td>\n",
       "      <td>Облава (фильм, 2012)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128158-N</td>\n",
       "      <td>Шаньдун</td>\n",
       "      <td>Шаньдунский полуостров</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143699-N</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>Оксюморон</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4061-N</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>Доверенность</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136544-N</td>\n",
       "      <td>Шахматная доска</td>\n",
       "      <td>не связан</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>113701-N</td>\n",
       "      <td>Бог</td>\n",
       "      <td>Дэва (индуизм)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>124807-N</td>\n",
       "      <td>Наставничество</td>\n",
       "      <td>Ментор (город, Миннесота)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>9199-N</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>Трансурановые элементы</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>103410-N</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>Эсватини</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>141998-N</td>\n",
       "      <td>Землянка</td>\n",
       "      <td>Землянка (приток Маралихи)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     synset_id         wiki_title_gold                  wiki_title  multi\n",
       "0     115195-N                  Облава        Облава (фильм, 2012)      1\n",
       "1     128158-N                 Шаньдун      Шаньдунский полуостров      1\n",
       "2     143699-N               Оксюморон                   Оксюморон      0\n",
       "3       4061-N            Доверенность                Доверенность      1\n",
       "4     136544-N         Шахматная доска                   не связан      1\n",
       "...        ...                     ...                         ...    ...\n",
       "1725  113701-N                     Бог              Дэва (индуизм)      0\n",
       "1726  124807-N          Наставничество   Ментор (город, Миннесота)      0\n",
       "1727    9199-N  Трансурановые элементы      Трансурановые элементы      1\n",
       "1728  103410-N                Эсватини                    Эсватини      0\n",
       "1729  141998-N                Землянка  Землянка (приток Маралихи)      1\n",
       "\n",
       "[1730 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_apply = df[\"synset_id\"].copy()\n",
    "dataset[\"multi\"] = for_apply.apply(lambda x: 0 if x in set_single else 1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482035928143712"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 0, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 0, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4378531073446328"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = dataset.loc[dataset['multi'] == 1, 'wiki_title_gold'].values\n",
    "y_pred = dataset.loc[dataset['multi'] == 1, 'wiki_title'].values\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3043d1795ba4e0c21ad8f92575aa91bd2aeb4e429e5ce129ef86fd0e27798542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
